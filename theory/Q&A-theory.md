Отлично, я проанализировал вашу шпаргалку. Ваши ответы точны, но часто используют аналогии вместо строгих определений, что делает их отличными для запоминания, но не всегда подходит для строгой проверки знаний. Я скорректировал свои предыдущие ответы, сделав их краткими, точными и дополнив их там, где это необходимо, чтобы они соответствовали академическому стилю, но при этом сохранили ясность.

Вот итоговая таблица со скорректированными вопросами и ответами.

| Вопрос | Краткий ответ |
| :--- | :--- |
| **Автоэнкодеры (AE)** | |
| 1. Чем отличается архитектура автоэнкодера от типичной многослойной нейронной сети, и какова роль "бутылочного горлышка" (bottleneck) в этой архитектуре? | Автоэнкодер состоит из двух частей: энкодера (сжатие) и декодера (восстановление). Цель — минимизировать разницу между входом и выходом. "Бутылочное горлышко" — это скрытый слой минимальной размерности, который выступает в роли регуляризатора, вынуждая модель изучать наиболее эффективное сжатое представление (признаки) входных данных. |
| 2. В чем заключается основная цель функции потерь при обучении автоэнкодера, и почему для задач реконструкции часто используется среднеквадратичная ошибка (MSE) или бинарная кросс-энтропия? | Цель функции потерь — количественно оценить ошибку реконструкции, то есть разницу между исходным входом и восстановленным выходом. MSE используется для непрерывных данных (например, пиксели изображения), предполагая гауссово распределение ошибки. Бинарная кросс-энтропия применяется, когда входные данные интерпретируются как вероятности (например, после нормализации к [0,1]), и модель пытается восстановить это распределение. |
| 3. Объясните разницу между "undercomplete" и "overcomplete" (разреженным) автоэнкодером. Какие проблемы решает разреженный автоэнкодер? | Undercomplete AE имеет скрытое представление меньшей размерности, чем вход, что автоматически приводит к сжатию. Overcomplete AE имеет скрытое представление большей размерности и может выучить тривиальную функцию тождества (копирование). Разреженный (Sparse) AE решает эту проблему, добавляя штраф за активность нейронов в скрытом слое, что заставляет его находить полезные, интерпретируемые признаки даже при большой размерности. |
| **Механизм внимания (Attention)** | |
| 1. Опишите основную идею механизма внимания: зачем нужно вычислять "веса внимания" и на какие три компонента (Query, Key, Value) опирается эта операция? | Идея в том, чтобы позволить модели динамически фокусироваться на наиболее релевантных частях входных данных при обработке каждого элемента. Query (запрос) ищет соответствия среди Key (ключей), на основе сходства вычисляются веса внимания, которые затем используются для взвешенного суммирования Value (значений), получая итоговый контекстно-зависимый вектор. |
| 2. В чем заключается ключевое отличие многоголового внимания (Multi-Head Attention) от одноголового? Какие преимущества дает использование нескольких "голов"? | Многоголовое внимание выполняет несколько операций внимания параллельно с разными, обучаемыми линейными проекциями Query, Key, Value. Каждая "голова" может учиться фокусироваться на разных типах взаимосвязей (например, разные расстояния в предложении, разные семантические отношения), что позволяет модели собирать более богатую информацию из разных подпространств представлений. |
| 3. Почему в архитектуре Трансформера используется позиционное кодирование (positional encoding) и как оно влияет на обработку последовательности слов? | Механизм самовнимания является инвариантным к перестановкам (permutation-invariant), то есть обрабатывает последовательность как неупорядоченное множество. Позиционное кодирование добавляет к входным эмбеддингам информацию о позиции токена в последовательности, позволяя модели учитывать порядок слов, который критически важен для синтаксиса и семантики языка. |
| **Резервуарные вычислительные сети (RC)** | |
| 1. В чем заключается главная особенность обучения резервуарных сетей (например, Echo State Networks) по сравнению с классическими RNN? Какие веса обучаются, а какие фиксированы? | Главная особенность — обучение без обратного распространения ошибки во времени. Только веса выходного слоя (линейная регрессия на состояниях резервуара) являются обучаемыми. Веса входного слоя и рекуррентные связи внутри резервуара задаются случайным образом и остаются фиксированными. |
| 2. Что такое "эхо-состояние" (echo state property) и почему оно критически важно для работы резервуарной сети? | Это свойство гарантирует, что состояние резервуара является "эхом" (затухающей функцией) истории входных сигналов, а влияние начального состояния сети со временем стремится к нулю. Оно необходимо для обеспечения устойчивости и воспроизводимости работы сети: при одинаковой входной последовательности состояния резервуара в конечном итоге сойдутся, независимо от начальных условий. |
| 3. Назовите основные преимущества и недостатки резервуарных сетей при прогнозировании временных рядов перед LSTM. | Преимущества: крайне быстрое и простое обучение (решение линейной задачи), отсутствие проблем с исчезающим/взрывающимся градиентом. Недостатки: потенциально более низкая точность на сложных, долговременных зависимостях по сравнению с LSTM; необходимость тщательного подбора гиперпараметров резервуара (размер, спектральный радиус, разреженность) для достижения хороших результатов. |
| **Сети Колмогорова-Арнольда (KAN)** | |
| 1. На каком математическом утверждении (теореме) базируется архитектура KAN, и чем оно отличается от теоремы об универсальной аппроксимации, на которой основаны MLP? | KAN базируется на теореме Колмогорова-Арнольда, которая утверждает, что любую непрерывную многомерную функцию можно представить как конечную композицию сумм одномерных функций. Теорема об универсальной аппроксимации (для MLP) говорит о существовании аппроксимации, но не предписывает конкретной архитектуры. KAN предлагает конкретную архитектуру, следующую из теоремы. |
| 2. В чем концептуальная разница между узлами (нейронами) в MLP и обучаемыми функциями на ребрах (связях) в сети KAN? | В MLP узлы (нейроны) содержат фиксированную нелинейную функцию активации, применяемую к взвешенной сумме входов (веса на ребрах — это скаляры). В KAN на ребрах (связях) находятся обучаемые одномерные функции (например, сплайны), а узлы просто суммируют входящие сигналы. Обучаются не скалярные веса, а параметры, определяющие форму этих функций на ребрах. |
| 3. Какое потенциальное преимущество KAN может дать в задачах аппроксимации функций с точки зрения интерпретируемости модели и точности при небольшом количестве параметров? | Потенциальные преимущества: 1) Интерпретируемость — обученные функции на ребрах можно визуализировать и анализировать, понимая, какую операцию выполняет сеть. 2) Точность и эффективность — благодаря гибкости сплайнов, KAN могут аппроксимировать сложные функции с меньшим количеством параметров, чем MLP, что особенно полезно для научных открытий. |
| **Графовые нейронные сети (GNN)** | |
| 1. Как GNN обобщают операцию свертки (как в CNN) для работы с нерегулярными структурами данных, такими как графы? | Вместо применения фильтра с фиксированным размером к регулярной сетке (как в CNN), GNN определяют свертку через агрегацию информации от соседей узла. "Свертка" для узла — это функция от признаков самого узла и признаков его соседей, порядок которых не важен (используются симметричные функции агрегации, такие как сумма или среднее). |
| 2. Объясните принцип работы механизма "передачи сообщений" (message passing) между узлами графа. | На каждом шаге каждый узел собирает "сообщения" от своих соседей. Сообщение — это преобразованный вектор признаков соседа. Затем все собранные сообщения агрегируются (суммируются, усредняются), и на основе агрегированной информации и своего текущего состояния узел обновляет свое скрытое представление через обучаемую функцию обновления. |
| 3. Почему GNN особенно полезны для анализа коннектома мозга (brain connectome)? Какую информацию о взаимодействии областей мозга они могут извлечь? | Коннектом мозга по своей сути является графом. GNN могут моделировать, как структура связей (ребра) влияет на активность и взаимодействие между областями (узлы). Они способны выявлять биомаркеры заболеваний, классифицировать пациентов на основе структуры их коннектома и предсказывать эволюцию связей, что невозможно при анализе областей по отдельности. |
| **Большие языковые модели (LLM)** | |
| 1. Опишите основную архитектуру современных LLM (трансформер) и роль компонента "самовнимания" (self-attention) в понимании контекста. | Современные LLM базируются на архитектуре Трансформера, состоящей из стека блоков энкодера и/или декодера. Ключевой компонент — самовнимание (self-attention), которое позволяет каждому токену "взаимодействовать" со всеми остальными токенами в последовательности, вычисляя степень их взаимного влияния для создания контекстно-зависимых эмбеддингов. |
| 2. В чем разница между этапами предобучения (pre-training) и тонкой настройки (fine-tuning) языковой модели? | Предобучение — это обучение модели на огромных объемах неразмеченных текстовых данных с использованием самообучающихся задач (например, предсказание следующего слова или маскированного токена). Цель — выучить универсальные представления языка. Тонкая настройка — это дополнительное обучение предобученной модели на небольшом размеченном датасете для конкретной задачи (например, анализ тональности или вопросно-ответная система). |
| 3. Что такое "контекстное обучение" (in-context learning / few-shot) и как оно реализуется в LLM без изменения весов модели? | Это способность LLM решать новую задачу, просто получая в качестве входного контекста (промпта) несколько демонстрационных примеров (few-shot) или подробное описание задачи. Модель, не обновляя свои веса, использует понимание структуры языка и паттернов из промпта, чтобы предсказать продолжение для нового тестового примера. |
| **AutoGluon Chronos (LLM + Time Series)** | |
| 1. В чем заключается инновация подхода AutoGluon Chronos к прогнозированию временных рядов? Как он использует архитектуру, схожую с LLM? | Инновация в том, что Chronos переформулирует задачу прогнозирования временных рядов как задачу моделирования языка. Он использует архитектуру Трансформера (энкодер) и обучается на огромном количестве разнообразных временных рядов, представленных в виде последовательности дискретных токенов, предсказывая следующий токен. |
| 2. Как происходит токенизация временного ряда для подачи в модель, изначально предназначенную для работы с текстом? | Значения временного ряда сначала масштабируются (например, с помощью нормализации), а затем квантуются — каждое масштабированное значение округляется до ближайшего значения из фиксированного, заранее определенного словаря (например, словаря из 4096 возможных значений). Полученная последовательность токенов и подается на вход модели. |
| 3. Какое преимущество дает предобучение на большом количестве разнообразных временных рядов (масштабирование) по сравнению с классическими статистическими методами вроде ARIMA/SARIMA? | Главное преимущество — способность к zero-shot и few-shot прогнозированию. Предобученная на разнообразных данных модель (Chronos) уже знает множество типичных паттернов (тренды, сезонность, циклы). Для прогноза на новом ряду она может сразу давать качественный результат (zero-shot) или требовать лишь минимальной донастройки, в то время как ARIMA нужно обучаться с нуля на каждом целевом ряду. |
| **Основы нейросетей и цифровая медицина** | |
| 1. Какие этические и юридические проблемы (информационная безопасность, конфиденциальность) возникают при использовании ИИ в медицине, особенно при работе с данными пациентов? | Ключевые проблемы: обеспечение конфиденциальности и анонимизации данных (соответствие GDPR, HIPAA), риск предвзятости (bias) алгоритмов, усугубляющей неравенство в здравоохранении, проблема "черного ящика" (отсутствие интерпретируемости) и вопрос юридической ответственности за ошибки, допущенные при использовании ИИ-систем. |
| 2. Для каких типов задач в медицине (диагностика, прогнозирование, сегментация) ИИ показывает наибольшую эффективность? Приведите примеры. | Наибольшая эффективность достигается в задачах, где есть четкие паттерны в данных: компьютерное зрение (классификация дерматоскопических изображений, сегментация опухолей на МРТ), анализ сигналов (обнаружение аритмий на ЭКГ), прогностическая аналитика (предсказание риска развития сепсиса или повторной госпитализации на основе электронных медицинских карт). |
| 3. Почему платформы вроде Kaggle и PhysioNet важны для развития медицинского ИИ? Какую роль они играют для исследователей? | Они играют ключевую роль, предоставляя исследователям по всему миру доступ к большим, размеченным, стандартизированным датасетам (например, MIMIC, часы PhysioNet). Это позволяет разрабатывать и объективно сравнивать модели, воспроизводить результаты и ускоряет прогресс за счет соревновательного элемента (как на Kaggle), без необходимости тратить годы на сбор и аннотацию данных. |
| **Предобработка данных и визуализация** | |
| 1. Почему необходимо разделять данные на train, validation и test наборы? Какую функцию выполняет валидационная выборка в процессе обучения? | Это необходимо для объективной оценки способности модели к обобщению. Train используется для обучения весов. Validation используется для настройки гиперпараметров, выбора модели и ранней остановки (early stopping), помогая избежать переобучения под тестовые данные. Test используется только один раз в конце для финальной оценки качества модели на новых, невиданных ранее данных. |
| 2. Какие методы масштабирования признаков (нормализация, стандартизация) вы знаете, и в каких случаях применение каждого из них предпочтительнее? | Нормализация (Min-Max scaling) приводит признаки к диапазону [0, 1] и предпочтительна, если данные не имеют нормального распределения или нужно сохранить точные границы. Стандартизация (Z-score normalization) приводит признаки к нулевому среднему и единичному отклонению и лучше подходит, если данные имеют (приближенно) нормальное распределение или есть выбросы, так как она менее чувствительна к ним. |
| 3. Как визуализация с помощью seaborn может помочь обнаружить выбросы (аномалии) в медицинских данных до обучения модели? | Seaborn позволяет строить графики, на которых выбросы легко идентифицировать визуально: box plots показывают точки за пределами "усов" (обычно 1.5*IQR); scatter plots (диаграммы рассеяния) позволяют увидеть изолированные точки, далеко отстоящие от основного скопления данных; гистограммы и графики плотности (distplot, kdeplot) могут выявить "длинные хвосты" распределения. |
| **FFN и обратное распространение (Backpropagation)** | |
| 1. Опишите суть алгоритма обратного распространения ошибки. Как градиенты ошибки передаются от выходного слоя к входному? | Это метод вычисления градиента функции потерь по всем весам сети. Сначала выполняется прямой проход (forward pass) для вычисления ошибки. Затем, используя правило цепочки (chain rule) из математического анализа, градиент ошибки распространяется обратно от выходного слоя к входному, вычисляя вклад каждого веса в общую ошибку. |
| 2. В чем заключается проблема переобучения (overfitting) и как она проявляется на графиках ошибки обучения и валидации? | Модель слишком точно подстраивается под обучающие данные, запоминая шум и необобщаемые закономерности, что ухудшает ее работу на новых данных. На графиках это проявляется как постоянно убывающая ошибка на train, в то время как ошибка на validation достигает минимума и начинает расти — это сигнал к остановке обучения. |
| 3. Чем задача регрессии (например, предсказание давления) отличается от задачи бинарной классификации (например, болен/здоров) на уровне функции активации выходного слоя и функции потерь? | Для регрессии: выходной слой обычно линейный (без активации) или с активацией, не ограничивающей диапазон (например, ReLU), функция потерь — среднеквадратичная ошибка (MSE) или средняя абсолютная ошибка (MAE). Для бинарной классификации: выходной слой — сигмоида, которая преобразует выход в вероятность принадлежности к классу [0, 1], функция потерь — бинарная кросс-энтропия. |
| **Сверточные нейронные сети (CNN)** | |
| 1. В чем заключается операция свертки (convolution) применительно к изображению, и что такое ядро свертки (kernel/filter)? | Ядро свертки — это небольшая матрица обучаемых весов. Оно систематически скользит (свертывается) по входному изображению или карте признаков. На каждой позиции выполняется операция поэлементного умножения значений ядра и соответствующей области входа, результаты суммируются, формируя одно число в выходной карте признаков. |
| 2. Зачем нужен слой подвыборки (pooling), и чем, например, MaxPooling отличается от AveragePooling? | Pooling уменьшает пространственную размерность карт признаков, что снижает количество параметров и вычислений, а также делает представления инвариантными к небольшим сдвигам и искажениям. MaxPooling выбирает максимальное значение из окна, выделяя наиболее сильные активации. AveragePooling вычисляет среднее значение, сглаживая активации. |
| 3. Для решения каких задач в обработке медицинских изображений (рентген, МРТ, КТ) архитектуры на основе CNN применяются чаще всего? | CNN — основа современных методов компьютерного зрения в медицине. Они применяются для: классификации (определение наличия патологии на снимке), сегментации (выделение контуров органов или опухолей), обнаружения объектов (локализация аномалий, например, узелков в легких) и регистрации изображений (совмещение снимков). |
| **Рекуррентные нейронные сети (RNN)** | |
| 1. В чем заключается основная проблема классических RNN (исчезающий или взрывающийся градиент) и к каким последствиям для обучения это приводит? | При обратном распространении ошибки через множество временных шагов градиенты имеют тенденцию экспоненциально уменьшаться (исчезать) или расти (взрываться). Это делает обучение крайне нестабильным и не позволяет RNN эффективно изучать долговременные зависимости в последовательностях. |
| 2. Почему RNN (и их разновидности) являются естественным выбором для анализа ЭКГ или ЭЭГ? | Потому что ЭКГ и ЭЭГ — это временные ряды, последовательности данных, где порядок следования отсчетов критически важен. RNN спроектированы для обработки последовательностей, сохраняя скрытое состояние, которое выступает в роли "памяти" о предыдущих элементах последовательности, что позволяет учитывать контекст во времени. |
| 3. *(Исправленный вопрос)* Как устроена архитектура RNN с точки зрения развертывания (unfolding) во времени и разделения весов? | RNN можно представить как глубокую сеть, развернутую во времени: одна и та же рекуррентная ячейка (с одинаковыми весами) копируется для каждого временного шага. Каждая копия получает свой вход и скрытое состояние от предыдущей копии. Это разделение весов (weight sharing) во времени — ключевая особенность, позволяющая модели обрабатывать последовательности любой длины. |
| **LSTM и эффект памяти** | |
| 1. Какие механизмы (вентили/гейты) в LSTM позволяют эффективно бороться с проблемой долговременной зависимости и исчезающего градиента? | LSTM использует три вентиля: входной (input gate) контролирует, какая новая информация запишется в ячейку памяти; вентиль забывания (forget gate) решает, какую информацию из прошлого состояния ячейки нужно удалить; выходной (output gate) определяет, какая информация из ячейки будет передана на выход. Такая структура обеспечивает более плавное протекание градиента через время. |
| 2. В чем ключевое отличие архитектуры LSTM от GRU (Gated Recurrent Unit) с точки зрения количества вентилей и внутреннего состояния? | LSTM имеет три вентиля (вход, забывание, выход) и два состояния — скрытое состояние (hidden state) и состояние ячейки (cell state). GRU имеет два вентиля (обновления и сброса) и только одно состояние (hidden state), объединяя состояние ячейки и скрытое состояние. GRU проще и быстрее в вычислениях, но LSTM может быть более выразительным. |
| 3. Что такое двунаправленная RNN (BiRNN) и в каких случаях использование будущего контекста улучшает качество предсказаний? | BiRNN обрабатывает последовательность двумя независимыми RNN: одна — слева направо (прямая), другая — справа налево (обратная). Выходы обеих RNN на каждом шаге конкатенируются. Это позволяет использовать как прошлый, так и будущий контекст, что критически важно в задачах вроде распознавания речи, машинного перевода или аннотирования сигналов (например, всего сегмента ЭКГ). |
| **Метрики качества и улучшение моделей** | |
| 1. Почему accuracy (точность) может быть плохой метрикой для задачи диагностики редкого заболевания? Какие метрики (специфичность, чувствительность, F-мера) лучше использовать в этом случае и почему? | При сильном дисбалансе классов (например, 1% больных, 99% здоровых) модель, предсказывающая всегда "здоров", получит 99% accuracy, но будет бесполезна. Чувствительность (Sensitivity/Recall) показывает долю верно выявленных больных (истинно положительных). Специфичность (Specificity) — долю верно классифицированных здоровых (истинно отрицательных). F-мера — это гармоническое среднее между точностью (Precision) и полнотой (Recall), полезно для сравнения моделей в условиях дисбаланса. |
| 2. Объясните принцип работы метода dropout. Как этот метод помогает бороться с переобучением? | Во время обучения на каждом проходе случайным образом "выключается" (обнуляется) заданная доля нейронов в слое. Это предотвращает чрезмерную коадаптацию (co-adaptation) нейронов, заставляя сеть учиться более избыточным и обобщенным признакам. На этапе тестирования/инференса используется вся сеть, но выходы масштабируются. |
| 3. Для чего используется метод SMOTE при работе с несбалансированными медицинскими датасетами, и в чем заключается его основная идея? | SMOTE используется для борьбы с дисбалансом классов путем генерации синтетических примеров для миноритарного класса. Основная идея: для выбранного примера из миноритарного класса находится один из его k ближайших соседей (тоже из миноритарного класса), и новый пример создается путем линейной интерполяции между этими двумя точками в пространстве признаков. |
